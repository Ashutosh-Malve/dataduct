#!/usr/bin/env python
# PYTHON_ARGCOMPLETE_OK

"""Script that helps create and validate pipelines from command line
"""

import argparse
from argparse import RawTextHelpFormatter

from dataduct.config import Config
from dataduct.config import logger_configuration

import logging
logger = logging.getLogger(__name__)

CREATE_STR = 'create'
VALIDATE_STR = 'validate'
ACTIVATE_STR = 'activate'
VISUALIZE_STR = 'visualize'
DROP_STR = 'drop'
GRANT_STR = 'grant'
RECREATE_STR = 'recreate'

CONFIG_TO_S3 = 'sync_to_s3'
CONFIG_FROM_S3 = 'sync_from_s3'

CONFIG_COMMAND = 'config'
DATABASE_COMMAND = 'database'
PIPELINE_COMMAND = 'pipeline'
DEV = 'dev'

formatter_class = lambda prog: RawTextHelpFormatter(prog, max_help_position=50)


def config_actions(action, filename):
    """Config related actions are executed in this block
    """
    from dataduct.config.config_actions import sync_to_s3
    from dataduct.config.config_actions import sync_from_s3

    if action == CONFIG_TO_S3:
        return sync_to_s3()
    return sync_from_s3(filename)


def initialize_etl_objects(load_definitions, delay=None,
                           frequency_override=None):
    """Generate etl objects from yaml files
    """
    from dataduct.etl import create_pipeline
    from dataduct.etl import read_pipeline_definition

    etls = []
    for load_definition in load_definitions:
        definition = read_pipeline_definition(load_definition)
        if delay is not None:
            definition.update({'delay': delay})
        if frequency_override is not None:
            definition.update({'frequency': frequency_override})
        etls.append(create_pipeline(definition))
    return etls


def pipeline_actions(action, load_definitions, force_overwrite=None,
                     delay=None, frequency_override=None,
                     activities_only=None, filename=None):
    """Pipeline related actions are executed in this block
    """
    from dataduct.etl import activate_pipeline
    from dataduct.etl import validate_pipeline
    from dataduct.etl import visualize_pipeline

    for etl in initialize_etl_objects(load_definitions, delay,
                                      frequency_override):
        if action in [VALIDATE_STR, ACTIVATE_STR]:
            validate_pipeline(etl, force_overwrite)
        if action == ACTIVATE_STR:
            activate_pipeline(etl)
        if action == VISUALIZE_STR:
            visualize_pipeline(etl, activities_only, filename)


def database_actions(action, table_definitions, filename=None):
    """Database related actions are executed in this block
    """
    from dataduct.database import Database

    database = Database(files=table_definitions)
    if action == CREATE_STR:
        script = database.create_relations_script()
    elif action == DROP_STR:
        script = database.drop_relations_script()
    elif action == GRANT_STR:
        script = database.grant_relations_script()
    elif action == RECREATE_STR:
        script = database.recreate_relations_script()
    elif action == VISUALIZE_STR:
        database.visualize(filename)
        script = ''
    print script


class _HelpAction(argparse._HelpAction):
    """HelpAction class used to render a custom help message
    """
    def __call__(self, parser, namespace, values, option_string=None):
        parser.print_help()
        print ''
        # retrieve subparsers from parser
        subparsers_actions = [
            action for action in parser._actions
            if isinstance(action, argparse._SubParsersAction)]

        for subparsers_action in subparsers_actions:
            # get all subparsers and print help
            for choice, subparser in subparsers_action.choices.items():
                print "Command '{}'".format(choice)
                print subparser.format_usage()
        parser.exit()


def choice_help_formatter(choices):
    """Create help format for a choice argument
    """
    result = []
    for key, value in choices.iteritems():
        result.append('%s: %s' % (key, value))
    return '\n'.join(result)


def main():
    """Main function
    """

    # Overwrite default help
    help_parser = argparse.ArgumentParser(
        description='Help message',
        add_help=False)
    help_parser.add_argument(
        '-h',
        '--help',
        action=_HelpAction,
        help='Help message',
    )

    # Base parser for all commands
    base_parser = argparse.ArgumentParser(
        description='Base Parser',
        add_help=False,
    )
    base_parser.add_argument(
        '-F',
        '--filename',
        default=None,
        help='Filename to store output of commands',
    )
    base_parser.add_argument(
        '-m',
        '--mode',
        default=None,
        help='Mode to run the pipeline and config overrides to use',
    )

    # Main parser
    parser = argparse.ArgumentParser(
        description='Run Dataduct commands',
        add_help=False,
        parents=[help_parser],
        formatter_class=formatter_class,
    )
    subparsers = parser.add_subparsers(
        dest='command',
        help='Actions for various features')

    # Config parser declaration
    config_choices = {
        CONFIG_TO_S3: 'sync config file from local to s3',
        CONFIG_FROM_S3: 'sync config file from s3 to local file',
    }
    config_parser = subparsers.add_parser(
        CONFIG_COMMAND,
        parents=[base_parser],
        formatter_class=formatter_class,
        help='Command to sync config to and from S3'
    )
    config_parser.add_argument(
        'action',
        type=str,
        choices=config_choices,
        help=choice_help_formatter(config_choices),
        default=CONFIG_FROM_S3,
    )

    # Pipeline parser declaration
    pipeline_choices = {
        CREATE_STR: 'Create a pipeline locally',
        VALIDATE_STR: 'Validate a pipeline with AWS without activating',
        ACTIVATE_STR: 'Activate the pipeline on AWS',
        VISUALIZE_STR: 'Visualize the pipeline',
    }
    pipeline_parser = subparsers.add_parser(
        PIPELINE_COMMAND,
        parents=[base_parser],
        formatter_class=formatter_class,
        help='Command for various operations on pipeline definitions',
    )
    pipeline_parser.add_argument(
        'action',
        type=str,
        choices=pipeline_choices,
        help=choice_help_formatter(pipeline_choices),
        default=CREATE_STR,
    )
    pipeline_parser.add_argument(
        'load_definitions',
        nargs='+',
        help='Enter the paths of the load definitions',
    )
    pipeline_parser.add_argument(
        '-f',
        '--force_overwrite',
        action='store_true',
        default=False,
        help='Indicates that if this pipeline exists, it will be destroyed',
    )
    pipeline_parser.add_argument(
        '-d',
        '--delay',
        default=0,
        type=int,
        help='Delay the pipeline by x days',
    )
    pipeline_parser.add_argument(
        '--activities_only',
        action='store_true',
        help='Visualize only activities',
    )

    # Database parser declaration
    database_choices = {
        CREATE_STR: 'Create tables',
        DROP_STR: 'Drop views and tables',
        GRANT_STR: 'Grant permissions to neccessary groups',
        RECREATE_STR: 'Recreate tables, load new data, drop old tables',
        VISUALIZE_STR: 'Visualize the database er-diagram',
    }
    database_parser = subparsers.add_parser(
        DATABASE_COMMAND,
        parents=[base_parser],
        formatter_class=formatter_class,
        help='Command for various operations on the database',
    )
    database_parser.add_argument(
        'action',
        type=str,
        choices=database_choices,
        help=choice_help_formatter(database_choices),
    )
    database_parser.add_argument(
        'table_definitions',
        nargs='+',
        help='Enter the paths of the table definitions',
    )

    # Check if autocomplete is possible
    try:
        import argcomplete
        argcomplete.autocomplete(parser)
    except ImportError:
        pass
    args = parser.parse_args()

    # Check if mode is dev
    mode = args.mode
    if mode is not None:
        # We assume mode:dev = mode:None
        if mode == DEV:
            mode = None

    # To instantiate the singleton object with the correct state
    # As this is the single entry point to the library
    # We can use the __new__ function to set the debug_level
    config = Config(mode=mode)

    # Setup up logging for package
    logger_configuration()

    if mode is not None:
        logger.warning('Running the pipeline in %s mode.' % config.mode)

    # Frequency override
    # Certain modes in the config can override frequency of a pipeline
    frequency_override = config.etl.get('FREQUENCY_OVERRIDE', None)

    # Action parse
    if args.command == CONFIG_COMMAND:
        config_actions(args.action, args.filename)
    elif args.command == PIPELINE_COMMAND:
        pipeline_actions(args.action, args.load_definitions,
                         args.force_overwrite, args.delay, frequency_override,
                         args.activities_only, args.filename)
    elif args.command == DATABASE_COMMAND:
        database_actions(args.action, args.table_definitions, args.filename)
    else:
        raise ValueError('Unknown argument provided, use dataduct')


if __name__ == '__main__':
    main()
