#!/usr/bin/env python

"""
Script that helps create and validate pipelines from command line
"""

import argparse

from dataduct.etl import activate_pipeline
from dataduct.etl import create_pipeline
from dataduct.etl import read_pipeline_definition
from dataduct.etl import validate_pipeline
from dataduct.etl import visualize_pipeline

from dataduct.config import sync_to_s3
from dataduct.config import sync_from_s3


CREATE_STR = 'create'
VALIDATE_STR = 'validate'
ACTIVATE_STR = 'activate'
VISUALIZE_STR = 'visualize'
SYNC_CONFIG_TO_S3 = 'sync_config_to_s3'
SYNC_CONFIG_FROM_S3 = 'sync_config_from_s3'

def main():
    """Main function"""
    parser = argparse.ArgumentParser(description='Run Dataduct commands')
    parser.add_argument(
        '-a',
        '--action',
        type=str,
        choices={
            CREATE_STR: 'Create a pipeline locally',
            VALIDATE_STR: 'Validate a pipeline with AWS without activating',
            ACTIVATE_STR: 'create a pipeline and activate it on AWS',
            VISUALIZE_STR: 'visualize a pipeline',
            SYNC_CONFIG_TO_S3: 'sync config file from local to s3',
            SYNC_CONFIG_FROM_S3: 'sync config file from s3 to local file',
        },
        default=CREATE_STR,
    )
    parser.add_argument(
        'load_definitions',
        nargs='*',
        help='Enter the paths of the load definitions.',
    )
    parser.add_argument(
        '-f',
        '--force_overwrite',
        action='store_true',
        default=False,
        help='Indicates that if this pipeline exists, it will be destroyed'
        ' first.',
    )
    parser.add_argument(
        '-filename',
        '--filename',
        default=None,
        help='Indicates that if this pipeline exists, it will be destroyed'
        ' first.',
    )
    args = parser.parse_args()

    if args.action == SYNC_CONFIG_TO_S3:
        return sync_to_s3()

    if args.action == SYNC_CONFIG_FROM_S3:
        return sync_from_s3(args.filename)

    for load_definition in args.load_definitions:
        definition = read_pipeline_definition(load_definition)
        etl = create_pipeline(definition)
        if args.action in [VISUALIZE_STR]:
            visualize_pipeline(etl, args.filename)
        if args.action in [VALIDATE_STR, ACTIVATE_STR]:
            validate_pipeline(etl, args.force_overwrite)
        if args.action == ACTIVATE_STR:
            activate_pipeline(etl)


if __name__ == '__main__':
    main()
