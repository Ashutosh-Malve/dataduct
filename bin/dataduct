#!/usr/bin/env python

"""
Script that helps create and validate pipelines from command line
"""

import argparse

from dataduct.config import Config

CREATE_STR = 'create'
VALIDATE_STR = 'validate'
ACTIVATE_STR = 'activate'

CONFIG_TO_S3 = 'sync_to_s3'
CONFIG_FROM_S3 = 'sync_from_s3'

CONFIG_COMMAND = 'config'
PIPELINE_COMMAND = 'pipeline'
VISUALIZE_COMMAND = 'visualize'


def config_actions(action, filename):
    """Config related actions are executed in this block
    """
    from dataduct.config.config_actions import sync_to_s3
    from dataduct.config.config_actions import sync_from_s3

    if action == CONFIG_TO_S3:
        return sync_to_s3()

    if action == CONFIG_FROM_S3:
        return sync_from_s3(filename)


def pipeline_actions(action, load_definitions, force_overwrite, delay):
    """Pipeline related actions are executed in this block
    """
    from dataduct.etl import activate_pipeline
    from dataduct.etl import create_pipeline
    from dataduct.etl import read_pipeline_definition
    from dataduct.etl import validate_pipeline

    for load_definition in load_definitions:
        definition = read_pipeline_definition(load_definition)
        definition.update({'delay': delay})

        etl = create_pipeline(definition)
        if action in [VALIDATE_STR, ACTIVATE_STR]:
            validate_pipeline(etl, force_overwrite)
        if action == ACTIVATE_STR:
            activate_pipeline(etl)


def visualize_actions(load_definitions, filename):
    """Visualization actions are executed in this block
    """
    from dataduct.etl import create_pipeline
    from dataduct.etl import read_pipeline_definition
    from dataduct.etl import visualize_pipeline

    for load_definition in load_definitions:
        definition = read_pipeline_definition(load_definition)

        etl = create_pipeline(definition)
        visualize_pipeline(etl, filename)


def main():
    """Main function"""
    parser = argparse.ArgumentParser(description='Run Dataduct commands')

    parser.add_argument(
        '-m',
        '--mode',
        default=None,
        help='Mode to run the pipeline and config overrides to use',
    )

    subparsers = parser.add_subparsers(help='Commands', dest='command')

    # Config parser declaration
    config_parser = subparsers.add_parser(CONFIG_COMMAND)

    config_parser.add_argument(
        'action',
        type=str,
        choices={
            CONFIG_TO_S3: 'sync config file from local to s3',
            CONFIG_FROM_S3: 'sync config file from s3 to local file',
        },
        default=CONFIG_FROM_S3,
    )

    config_parser.add_argument(
        '-f',
        '--filename',
        default=None,
        help='Filename to sync',
    )

    # Pipeline parser declaration
    pipeline_parser = subparsers.add_parser(PIPELINE_COMMAND)

    pipeline_parser.add_argument(
        'action',
        type=str,
        choices={
            CREATE_STR: 'Create a pipeline locally',
            VALIDATE_STR: 'Validate a pipeline with AWS without activating',
            ACTIVATE_STR: 'create a pipeline and activate it on AWS',
        },
        default=CREATE_STR,
    )

    pipeline_parser.add_argument(
        'load_definitions',
        nargs='*',
        help='Enter the paths of the load definitions',
    )

    pipeline_parser.add_argument(
        '-f',
        '--force_overwrite',
        action='store_true',
        default=False,
        help='Indicates that if this pipeline exists, it will be destroyed',
    )

    pipeline_parser.add_argument(
        '-d',
        '--delay',
        default=0,
        type=int,
        help='Delay the pipeline by x days',
    )

    # Visualize parser declaration
    visualize_parser = subparsers.add_parser(VISUALIZE_COMMAND)

    visualize_parser.add_argument(
        'filename',
        help='Filename for the graph',
    )

    visualize_parser.add_argument(
        'load_definitions',
        nargs='*',
        help='Enter the paths of the load definitions',
    )

    args = parser.parse_args()

    mode = args.mode
    if mode is not None:
        # We assume mode:dev = mode:None
        if mode == 'dev':
            mode = None

        # To instantiate the singleton object with the correct state
        # As this is the single entry point to the library
        # We can use the __new__ function to set the debug_level
        config = Config(mode=mode)
        print 'Running the pipeline in %s mode.' % config.mode

    if args.command == CONFIG_COMMAND:
        config_actions(args.action, args.filename)
    elif args.command == PIPELINE_COMMAND:
        pipeline_actions(args.action, args.load_definitions,
                         args.force_overwrite, args.delay)
    else:
        visualize_actions(args.load_definitions, args.filename)


if __name__ == '__main__':
    main()
