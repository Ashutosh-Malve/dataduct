#!/usr/bin/env python

"""Script that helps create and validate pipelines from command line
"""

import argparse

from dataduct.config import Config
from dataduct.config import logger_configuration

import logging
logger = logging.getLogger(__name__)

CREATE_STR = 'create'
VALIDATE_STR = 'validate'
ACTIVATE_STR = 'activate'
DROP_STR = 'drop'
GRANT_STR = 'grant'
RECREATE_STR = 'recreate'

CONFIG_TO_S3 = 'sync_to_s3'
CONFIG_FROM_S3 = 'sync_from_s3'

CONFIG_COMMAND = 'config'
DATABASE_COMMAND = 'database'
PIPELINE_COMMAND = 'pipeline'
VISUALIZE_COMMAND = 'visualize'

DEV = 'dev'

def config_actions(action, filename):
    """Config related actions are executed in this block
    """
    from dataduct.config.config_actions import sync_to_s3
    from dataduct.config.config_actions import sync_from_s3

    if action == CONFIG_TO_S3:
        return sync_to_s3()
    return sync_from_s3(filename)


def initialize_etl_objects(load_definitions, delay=None,
                           frequency_override=None):
    """Generate etl objects from yaml files
    """
    from dataduct.etl import create_pipeline
    from dataduct.etl import read_pipeline_definition

    etls = []
    for load_definition in load_definitions:
        definition = read_pipeline_definition(load_definition)
        if delay is not None:
            definition.update({'delay': delay})
        if frequency_override is not None:
            definition.update({'frequency': frequency_override})
        etls.append(create_pipeline(definition))
    return etls


def pipeline_actions(action, load_definitions, force_overwrite, delay,
                     frequency_override=None):
    """Pipeline related actions are executed in this block
    """
    from dataduct.etl import activate_pipeline
    from dataduct.etl import validate_pipeline

    for etl in initialize_etl_objects(load_definitions, delay,
                                      frequency_override):
        if action in [VALIDATE_STR, ACTIVATE_STR]:
            validate_pipeline(etl, force_overwrite)
        if action == ACTIVATE_STR:
            activate_pipeline(etl)


def database_actions(action, table_definitions):
    """Database related actions are executed in this block
    """
    from dataduct.database import Database

    database = Database(files=table_definitions)
    if action == CREATE_STR:
        script = database.create_relations_script()
    elif action == DROP_STR:
        script = database.drop_relations_script()
    elif action == GRANT_STR:
        script = database.grant_relations_script()
    elif action == RECREATE_STR:
        script = database.recreate_relations_script()
    print script


def visualize_pipeline_actions(load_definitions, activities_only, filename):
    """Visualization actions for pipelines are executed in this block
    """

    from dataduct.etl import visualize_pipeline

    for etl in initialize_etl_objects(load_definitions):
        visualize_pipeline(etl, activities_only, filename)


def visualize_database_actions(table_definitions, filename):
    """Visualization actions for databases are executed in this block
    """

    from dataduct.database import Database

    database = Database(files=table_definitions)
    database.visualize(filename)


class _HelpAction(argparse._HelpAction):
    """HelpAction class used to render a custom help message
    """
    def __call__(self, parser, namespace, values, option_string=None):
        parser.print_help()
        print ''

        # retrieve subparsers from parser
        subparsers_actions = [
            action for action in parser._actions
            if isinstance(action, argparse._SubParsersAction)]

        for subparsers_action in subparsers_actions:
            # get all subparsers and print help
            for choice, subparser in subparsers_action.choices.items():
                print "Command '{}'".format(choice)
                print subparser.format_usage()

        parser.exit()


def main():
    """Main function"""
    parser = argparse.ArgumentParser(description='Run Dataduct commands',
                                     add_help=False)
    parser.add_argument(
        '-m',
        '--mode',
        default=None,
        help='Mode to run the pipeline and config overrides to use',
    )
    # Overwrite default help
    parser.add_argument(
        '-h',
        '--help',
        action=_HelpAction,
        help='Show this help message and exit',
    )
    subparsers = parser.add_subparsers(help='Commands', dest='command')

    # Config parser declaration
    config_parser = subparsers.add_parser(CONFIG_COMMAND)
    config_parser.add_argument(
        'action',
        type=str,
        choices={
            CONFIG_TO_S3: 'sync config file from local to s3',
            CONFIG_FROM_S3: 'sync config file from s3 to local file',
        },
        default=CONFIG_FROM_S3,
    )
    config_parser.add_argument(
        '-f',
        '--filename',
        default=None,
        help='Filename to sync',
    )

    # Pipeline parser declaration
    pipeline_parser = subparsers.add_parser(PIPELINE_COMMAND)
    pipeline_parser.add_argument(
        'action',
        type=str,
        choices={
            CREATE_STR: 'Create a pipeline locally',
            VALIDATE_STR: 'Validate a pipeline with AWS without activating',
            ACTIVATE_STR: 'create a pipeline and activate it on AWS',
        },
        default=CREATE_STR,
    )
    pipeline_parser.add_argument(
        'load_definitions',
        nargs='+',
        help='Enter the paths of the load definitions',
    )
    pipeline_parser.add_argument(
        '-f',
        '--force_overwrite',
        action='store_true',
        default=False,
        help='Indicates that if this pipeline exists, it will be destroyed',
    )
    pipeline_parser.add_argument(
        '-d',
        '--delay',
        default=0,
        type=int,
        help='Delay the pipeline by x days',
    )

    # Database parser declaration
    database_parser = subparsers.add_parser(DATABASE_COMMAND)
    database_parser.add_argument(
        'action',
        type=str,
        choices={
            CREATE_STR: 'Create tables',
            DROP_STR: 'Drop views and tables',
            GRANT_STR: 'Grant permissions to neccessary groups',
            RECREATE_STR: 'Recreate tables, load new data, drop old tables',
        },
    )
    database_parser.add_argument(
        'table_definitions',
        nargs='+',
        help='Enter the paths of the table definitions',
    )

    # Visualize parser declaration
    visualize_parser = subparsers.add_parser(VISUALIZE_COMMAND)
    visualize_subparsers = visualize_parser.add_subparsers(
        help='Commands', dest='visualize_command')

    # Visualize pipeline parser declaration
    visualize_pipeline_parser = \
        visualize_subparsers.add_parser(PIPELINE_COMMAND)
    visualize_pipeline_parser.add_argument(
        'filename',
        help='Filename for the graph',
    )
    visualize_pipeline_parser.add_argument(
        'load_definitions',
        nargs='+',
        help='Enter the paths of the load definitions',
    )
    visualize_pipeline_parser.add_argument(
        '--activities-only',
        action='store_true',
        help='Visualize only activities',
    )

    # Visualize database parser declaration
    visualize_database_parser = \
        visualize_subparsers.add_parser(DATABASE_COMMAND)
    visualize_database_parser.add_argument(
        'filename',
        help='Filename for the graph',
    )
    visualize_database_parser.add_argument(
        'table_definitions',
        nargs='+',
        help='Enter the paths of the table definitions',
    )

    args = parser.parse_args()

    mode = args.mode
    if mode is not None:
        # We assume mode:dev = mode:None
        if mode == DEV:
            mode = None

    # To instantiate the singleton object with the correct state
    # As this is the single entry point to the library
    # We can use the __new__ function to set the debug_level
    config = Config(mode=mode)

    # Setup up logging for package
    logger_configuration()

    if mode is not None:
        logger.warning('Running the pipeline in %s mode.' % config.mode)

    # Frequency override
    # Certain modes in the config can override frequency of a pipeline
    frequency_override = config.etl.get('FREQUENCY_OVERRIDE', None)

    if args.command == CONFIG_COMMAND:
        config_actions(args.action, args.filename)
    elif args.command == PIPELINE_COMMAND:
        pipeline_actions(args.action, args.load_definitions,
                         args.force_overwrite, args.delay, frequency_override)
    elif args.command == DATABASE_COMMAND:
        database_actions(args.action, args.table_definitions)
    else:
        if args.visualize_command == PIPELINE_COMMAND:
            visualize_pipeline_actions(
                args.load_definitions, args.activities_only, args.filename)
        else:
            visualize_database_actions(
                args.table_definitions, args.filename)


if __name__ == '__main__':
    main()
